{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll01hmVoOW3M",
        "outputId": "7bbc5f9b-5cf8-42cd-9779-9580bbac9f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.13.1 dill-0.3.6 huggingface-hub-0.16.4 multiprocess-0.70.14 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from datasets import Dataset, DatasetDict"
      ],
      "metadata": {
        "id": "3ExPmZ_1OsKd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def json_to_DatasetDict(file_name):\n",
        "    # read the json file\n",
        "    with open(f\"{file_name}.json\", \"r\") as json_file:\n",
        "        data = json.load(json_file)\n",
        "\n",
        "    # extracting element from the data variable\n",
        "    text = data[\"annotations\"][0][0]\n",
        "    tags = data[\"annotations\"][0][1][\"entities\"]\n",
        "    classes = data[\"classes\"]\n",
        "\n",
        "    classes_dict = {}\n",
        "    for i in range(len(classes)):\n",
        "        classes_dict[classes[i]] = i\n",
        "\n",
        "    tokens = []\n",
        "    named_tags = []\n",
        "\n",
        "    for tag in tags:\n",
        "        tokens.append(text[tag[0]:tag[1]])\n",
        "        named_tags.append(tag[2])\n",
        "\n",
        "    anno_dict = dict(zip(tokens, named_tags))\n",
        "\n",
        "    all_tokens = text.split(\" \")\n",
        "    named_labels = []\n",
        "    numbered_labels = []\n",
        "\n",
        "    for token in all_tokens:\n",
        "        if token not in anno_dict:\n",
        "            named_labels.append(\"NONE\")\n",
        "        else:\n",
        "            named_labels.append(anno_dict[token])\n",
        "\n",
        "    for label in named_labels:\n",
        "        numbered_labels.append(classes_dict[label])\n",
        "\n",
        "\n",
        "    # dividing the tokens into 10 token array\n",
        "    all_tokens_array = []\n",
        "    numbered_labels_array = []\n",
        "\n",
        "    temp1 = []\n",
        "    temp2 = []\n",
        "    for i in range(len(all_tokens)):\n",
        "        temp1.append(all_tokens[i])\n",
        "        temp2.append(numbered_labels[i])\n",
        "        if i%10 == 0 and i != 0:\n",
        "            all_tokens_array.append(temp1)\n",
        "            numbered_labels_array.append(temp2)\n",
        "            temp1 = []\n",
        "            temp2 = []\n",
        "    all_tokens_array.append(temp1)\n",
        "    numbered_labels_array.append(temp2)\n",
        "\n",
        "\n",
        "    # packing necessary elements together to make the dict\n",
        "    # train, val, test division\n",
        "    list_len = len(all_tokens_array)\n",
        "    train_split = int(list_len*0.8)\n",
        "    val_and_test_split = int(list_len*0.1)\n",
        "\n",
        "    data_dict_train = {\n",
        "    \"tokens\" : all_tokens_array[0:train_split],\n",
        "    \"labels\" : numbered_labels_array[0:train_split]\n",
        "    }\n",
        "\n",
        "    data_dict_val = {\n",
        "        \"tokens\" : all_tokens_array[train_split:train_split+val_and_test_split],\n",
        "        \"labels\" : numbered_labels_array[train_split:train_split+val_and_test_split]\n",
        "    }\n",
        "\n",
        "    data_dict_test = {\n",
        "        \"tokens\" : all_tokens_array[train_split+val_and_test_split:],\n",
        "        \"labels\" : numbered_labels_array[train_split+val_and_test_split:]\n",
        "    }\n",
        "\n",
        "    dataset_train = Dataset.from_dict(data_dict_train)\n",
        "    dataset_val = Dataset.from_dict(data_dict_val)\n",
        "    dataset_test = Dataset.from_dict(data_dict_test)\n",
        "\n",
        "    dataset_dict = DatasetDict({\"train\": dataset_train, \"validate\": dataset_val, \"test\": dataset_test})\n",
        "\n",
        "    return dataset_dict"
      ],
      "metadata": {
        "id": "iRlvGltUOsHl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set_dict = json_to_DatasetDict(\"annotations\")\n",
        "print(data_set_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2miz70QOrxj",
        "outputId": "921d0606-a616-4799-e48d-74e692b6439f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['tokens', 'labels'],\n",
            "        num_rows: 200\n",
            "    })\n",
            "    validate: Dataset({\n",
            "        features: ['tokens', 'labels'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['tokens', 'labels'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AEbZ1GoORdTA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}